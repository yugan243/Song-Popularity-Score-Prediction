{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Proprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from ../data/raw/train.csv\n",
      "Loading test data from ../data/raw/test.csv\n",
      "Train shape: (61609, 62)\n",
      "Test shape: (41074, 61)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils.utils import load_config\n",
    "from src.data.load_data import load_data\n",
    "\n",
    "config = load_config('../configs/config.yaml')\n",
    "train_df, test_df = load_data(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Impute using KNN and Mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 52 numerical columns and 9 categorical columns for imputation.\n",
      "\n",
      "[INFO] Starting KNN imputation for numerical features...\n",
      "[INFO] KNN imputation for numerical features completed in 1166.77 seconds.\n",
      "\n",
      "[INFO] Starting mode imputation for categorical features...\n",
      "[INFO] Mode imputation for categorical features completed in 0.39 seconds.\n",
      "\n",
      "[INFO] Top columns with missing values in train after imputation:\n",
      "id                     0\n",
      "emotional_charge_2     0\n",
      "groove_efficiency_1    0\n",
      "beat_frequency_1       0\n",
      "organic_texture_2      0\n",
      "composition_label_0    0\n",
      "harmonic_scale_1       0\n",
      "intensity_index_0      0\n",
      "duration_ms_0          0\n",
      "album_name_length      0\n",
      "dtype: int64\n",
      "\n",
      "[INFO] Top columns with missing values in test after imputation:\n",
      "id                     0\n",
      "emotional_charge_2     0\n",
      "groove_efficiency_1    0\n",
      "beat_frequency_1       0\n",
      "organic_texture_2      0\n",
      "composition_label_0    0\n",
      "harmonic_scale_1       0\n",
      "intensity_index_0      0\n",
      "duration_ms_0          0\n",
      "album_name_length      0\n",
      "dtype: int64\n",
      "\n",
      "[INFO] Saving preprocessed datasets...\n",
      "[INFO] DataFrame saved to ../data/processed\\train_processed.csv\n",
      "[INFO] DataFrame saved to ../data/processed\\test_processed.csv\n",
      "[INFO] Preprocessed datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "import time\n",
    "from src.utils.utils import load_config, save_dataframe\n",
    "import os\n",
    "\n",
    "# 1. Identify numerical and categorical columns\n",
    "num_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "if 'target' in num_cols:\n",
    "    num_cols.remove('target')\n",
    "\n",
    "print(f\"Identified {len(num_cols)} numerical columns and {len(cat_cols)} categorical columns for imputation.\")\n",
    "\n",
    "# 2. KNN Imputation for numerical columns\n",
    "print(\"\\n[INFO] Starting KNN imputation for numerical features...\")\n",
    "start_time = time.time()\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
    "train_df_imputed = train_df.copy()\n",
    "test_df_imputed = test_df.copy()\n",
    "\n",
    "train_df_imputed[num_cols] = imputer.fit_transform(train_df[num_cols])\n",
    "test_df_imputed[num_cols] = imputer.transform(test_df[num_cols])\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"[INFO] KNN imputation for numerical features completed in {elapsed:.2f} seconds.\")\n",
    "\n",
    "# 3. Mode imputation for categorical columns\n",
    "print(\"\\n[INFO] Starting mode imputation for categorical features...\")\n",
    "start_time = time.time()\n",
    "for col in cat_cols:\n",
    "    mode = train_df[col].mode()[0]\n",
    "    train_df_imputed[col] = train_df[col].fillna(mode)\n",
    "    test_df_imputed[col] = test_df[col].fillna(mode)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"[INFO] Mode imputation for categorical features completed in {elapsed:.2f} seconds.\")\n",
    "\n",
    "# 4. Check missing values after imputation\n",
    "print(\"\\n[INFO] Top columns with missing values in train after imputation:\")\n",
    "print(train_df_imputed.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\n[INFO] Top columns with missing values in test after imputation:\")\n",
    "print(test_df_imputed.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# 5. Save preprocessed data\n",
    "print(\"\\n[INFO] Saving preprocessed datasets...\")\n",
    "config = load_config('../configs/config.yaml')\n",
    "processed_dir = config['data']['processed_dir']\n",
    "\n",
    "# Define save paths\n",
    "train_save_path = os.path.join(processed_dir, \"train_processed.csv\")\n",
    "test_save_path = os.path.join(processed_dir, \"test_processed.csv\")\n",
    "\n",
    "# Save DataFrames\n",
    "save_dataframe(train_df_imputed, train_save_path)\n",
    "save_dataframe(test_df_imputed, test_save_path)\n",
    "\n",
    "print(\"[INFO] Preprocessed datasets saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encode Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'composition_label_0',\n",
    "    'composition_label_1',\n",
    "    'weekday_of_release',\n",
    "    'season_of_release',\n",
    "    'lunar_phase',\n",
    "    'creator_collective',\n",
    "    'composition_label_2',\n",
    "    'track_identifier'\n",
    "]\n",
    "\n",
    "high_card_cols = [\n",
    "    'composition_label_0',\n",
    "    'composition_label_1',\n",
    "    'composition_label_2',\n",
    "    'creator_collective',\n",
    "    'track_identifier'\n",
    "]\n",
    "\n",
    "low_card_cols = [\n",
    "    'weekday_of_release',\n",
    "    'season_of_release',\n",
    "    'lunar_phase',\n",
    "    'release_period_in_month'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Smooth Target Encording for High-Cardinality Columns + One Hot Encoding for Low-Cardinality Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from ../data/processed/train_new1.csv\n",
      "Loading test data from ../data/processed/test_new1.csv\n",
      "Train shape: (61609, 83)\n",
      "Test shape: (41074, 82)\n",
      "[INFO] DataFrame saved to ../data/processed\\train_encoded.csv\n",
      "[INFO] DataFrame saved to ../data/processed\\test_encoded.csv\n",
      "[INFO] Encoded datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data.preprocess import SmoothedTargetEncoder\n",
    "from src.data.load_data import load_data\n",
    "from src.utils.utils import save_dataframe\n",
    "import os\n",
    "\n",
    "train_df, test_df = load_data(train_path=\"../data/processed/train_new1.csv\",\n",
    "                              test_path=\"../data/processed/test_new1.csv\"\n",
    "                              )\n",
    "\n",
    "target_col = 'target'\n",
    "\n",
    "# Apply smoothed target encoding\n",
    "for col in high_card_cols:\n",
    "    encoder = SmoothedTargetEncoder(column=col, smoothing=10)\n",
    "    train_df = encoder.fit_transform(train_df, train_df[target_col])\n",
    "    test_df = encoder.transform(test_df)\n",
    "    \n",
    "# Apply OHE \n",
    "train_df = pd.get_dummies(train_df, columns=low_card_cols)\n",
    "test_df = pd.get_dummies(test_df, columns=low_card_cols)\n",
    "\n",
    "# Align columns to ensure train and test have the same features\n",
    "train_df, test_df = train_df.align(test_df, join='left', axis=1, fill_value=0)\n",
    "test_df = test_df.drop(columns=['target'])\n",
    "\n",
    "\n",
    "# Convert boolean columns to integers (0/1)\n",
    "bool_cols_train = train_df.select_dtypes(include='bool').columns\n",
    "train_df[bool_cols_train] = train_df[bool_cols_train].astype(int)\n",
    "bool_cols_test = test_df.select_dtypes(include='bool').columns\n",
    "test_df[bool_cols_test] = test_df[bool_cols_test].astype(int)\n",
    "\n",
    "# Save the encoded data\n",
    "config = load_config('../configs/config.yaml')\n",
    "processed_dir = config['data']['processed_dir']\n",
    "\n",
    "train_save_path = os.path.join(processed_dir, \"train_encoded.csv\")\n",
    "test_save_path = os.path.join(processed_dir, \"test_encoded.csv\")\n",
    "\n",
    "save_dataframe(train_df, train_save_path)\n",
    "save_dataframe(test_df, test_save_path)\n",
    "\n",
    "print(\"[INFO] Encoded datasets saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61609 entries, 0 to 61608\n",
      "Data columns (total 97 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   Unnamed: 0                          61609 non-null  int64  \n",
      " 1   id                                  61609 non-null  float64\n",
      " 2   emotional_charge_2                  61609 non-null  float64\n",
      " 3   groove_efficiency_1                 61609 non-null  float64\n",
      " 4   beat_frequency_1                    61609 non-null  float64\n",
      " 5   organic_texture_2                   61609 non-null  float64\n",
      " 6   harmonic_scale_1                    61609 non-null  float64\n",
      " 7   intensity_index_0                   61609 non-null  float64\n",
      " 8   duration_ms_0                       61609 non-null  float64\n",
      " 9   album_name_length                   61609 non-null  float64\n",
      " 10  beat_frequency_0                    61609 non-null  float64\n",
      " 11  beat_frequency_2                    61609 non-null  float64\n",
      " 12  artist_count                        61609 non-null  float64\n",
      " 13  album_component_count               61609 non-null  float64\n",
      " 14  emotional_charge_1                  61609 non-null  float64\n",
      " 15  emotional_charge_0                  61609 non-null  float64\n",
      " 16  tonal_mode_2                        61609 non-null  float64\n",
      " 17  key_variety                         61609 non-null  float64\n",
      " 18  performance_authenticity_2          61609 non-null  float64\n",
      " 19  performance_authenticity_0          61609 non-null  float64\n",
      " 20  time_signature_1                    61609 non-null  float64\n",
      " 21  duration_ms_2                       61609 non-null  float64\n",
      " 22  instrumental_density_2              61609 non-null  float64\n",
      " 23  organic_texture_0                   61609 non-null  float64\n",
      " 24  vocal_presence_2                    61609 non-null  float64\n",
      " 25  tonal_mode_1                        61609 non-null  float64\n",
      " 26  vocal_presence_1                    61609 non-null  float64\n",
      " 27  vocal_presence_0                    61609 non-null  float64\n",
      " 28  intensity_index_1                   61609 non-null  float64\n",
      " 29  organic_immersion_0                 61609 non-null  float64\n",
      " 30  tonal_mode_0                        61609 non-null  float64\n",
      " 31  groove_efficiency_2                 61609 non-null  float64\n",
      " 32  instrumental_density_1              61609 non-null  float64\n",
      " 33  organic_immersion_2                 61609 non-null  float64\n",
      " 34  duration_consistency                61609 non-null  float64\n",
      " 35  organic_texture_1                   61609 non-null  float64\n",
      " 36  rhythmic_cohesion_0                 61609 non-null  float64\n",
      " 37  emotional_resonance_1               61609 non-null  float64\n",
      " 38  rhythmic_cohesion_1                 61609 non-null  float64\n",
      " 39  performance_authenticity_1          61609 non-null  float64\n",
      " 40  tempo_volatility                    61609 non-null  float64\n",
      " 41  organic_immersion_1                 61609 non-null  float64\n",
      " 42  groove_efficiency_0                 61609 non-null  float64\n",
      " 43  emotional_resonance_2               61609 non-null  float64\n",
      " 44  time_signature_0                    61609 non-null  float64\n",
      " 45  duration_ms_1                       61609 non-null  float64\n",
      " 46  harmonic_scale_0                    61609 non-null  float64\n",
      " 47  time_signature_2                    61609 non-null  float64\n",
      " 48  rhythmic_cohesion_2                 61609 non-null  float64\n",
      " 49  emotional_resonance_0               61609 non-null  float64\n",
      " 50  harmonic_scale_2                    61609 non-null  float64\n",
      " 51  intensity_index_2                   61609 non-null  float64\n",
      " 52  instrumental_density_0              61609 non-null  float64\n",
      " 53  target                              61609 non-null  int64  \n",
      " 54  release_year                        61609 non-null  int64  \n",
      " 55  release_month                       61609 non-null  int64  \n",
      " 56  release_day                         61609 non-null  int64  \n",
      " 57  release_dayofweek                   61609 non-null  int64  \n",
      " 58  release_quarter                     61609 non-null  int64  \n",
      " 59  week_of_year                        61609 non-null  int64  \n",
      " 60  day_of_year                         61609 non-null  int64  \n",
      " 61  is_weekend_release                  61609 non-null  int64  \n",
      " 62  is_summer_release                   61609 non-null  int64  \n",
      " 63  is_end_of_year                      61609 non-null  int64  \n",
      " 64  is_valentines_week                  61609 non-null  int64  \n",
      " 65  is_new_year_release                 61609 non-null  int64  \n",
      " 66  month_sin                           61609 non-null  float64\n",
      " 67  month_cos                           61609 non-null  float64\n",
      " 68  day_sin                             61609 non-null  float64\n",
      " 69  day_cos                             61609 non-null  float64\n",
      " 70  songs_released_that_year            61609 non-null  int64  \n",
      " 71  songs_released_that_month           61609 non-null  int64  \n",
      " 72  release_month_popularity_score      61609 non-null  float64\n",
      " 73  release_dayofweek_popularity_score  61609 non-null  float64\n",
      " 74  composition_label_0_encoded         61609 non-null  float64\n",
      " 75  composition_label_1_encoded         61609 non-null  float64\n",
      " 76  composition_label_2_encoded         61609 non-null  float64\n",
      " 77  creator_collective_encoded          61609 non-null  float64\n",
      " 78  track_identifier_encoded            61609 non-null  float64\n",
      " 79  weekday_of_release_Friday           61609 non-null  int64  \n",
      " 80  weekday_of_release_Monday           61609 non-null  int64  \n",
      " 81  weekday_of_release_Saturday         61609 non-null  int64  \n",
      " 82  weekday_of_release_Sunday           61609 non-null  int64  \n",
      " 83  weekday_of_release_Thursday         61609 non-null  int64  \n",
      " 84  weekday_of_release_Tuesday          61609 non-null  int64  \n",
      " 85  weekday_of_release_Wednesday        61609 non-null  int64  \n",
      " 86  season_of_release_autumn            61609 non-null  int64  \n",
      " 87  season_of_release_spring            61609 non-null  int64  \n",
      " 88  season_of_release_summer            61609 non-null  int64  \n",
      " 89  season_of_release_winter            61609 non-null  int64  \n",
      " 90  lunar_phase_full                    61609 non-null  int64  \n",
      " 91  lunar_phase_new                     61609 non-null  int64  \n",
      " 92  lunar_phase_waning                  61609 non-null  int64  \n",
      " 93  lunar_phase_waxing                  61609 non-null  int64  \n",
      " 94  release_period_in_month_early       61609 non-null  int64  \n",
      " 95  release_period_in_month_late        61609 non-null  int64  \n",
      " 96  release_period_in_month_mid         61609 non-null  int64  \n",
      "dtypes: float64(63), int64(34)\n",
      "memory usage: 45.6 MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from ../data/processed/train_encoded.csv\n",
      "Loading test data from ../data/processed/test_encoded.csv\n",
      "Train shape: (61609, 74)\n",
      "Test shape: (41074, 73)\n",
      "[INFO] DataFrame saved to ../data/processed\\train_standardized.csv\n",
      "[INFO] DataFrame saved to ../data/processed\\test_standardized.csv\n",
      "[INFO] Standardized datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# from src.data.preprocess import DataStandardizer\n",
    "# from src.utils.utils import load_config, save_dataframe\n",
    "# from src.data.load_data import load_data\n",
    "# import os\n",
    "\n",
    "# # 1. Load data\n",
    "# train, test = load_data(train_path=\"../data/processed/train_encoded.csv\",\n",
    "#                               test_path=\"../data/processed/test_encoded.csv\"\n",
    "#                               )\n",
    "\n",
    "# # 2. Initialize and fit on train\n",
    "# standardizer = DataStandardizer()\n",
    "# train_standardized = standardizer.fit_transform(train, target_col='target')\n",
    "# test_standardized = standardizer.transform(test)\n",
    "\n",
    "# # 3. Save standardized data\n",
    "# config = load_config('../configs/config.yaml')\n",
    "# processed_dir = config['data']['processed_dir']\n",
    "# train_save_path = os.path.join(processed_dir, \"train_standardized.csv\")\n",
    "# test_save_path = os.path.join(processed_dir, \"test_standardized.csv\")\n",
    "\n",
    "# save_dataframe(train_standardized, train_save_path)\n",
    "# save_dataframe(test_standardized, test_save_path)\n",
    "\n",
    "# print(\"[INFO] Standardized datasets saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
