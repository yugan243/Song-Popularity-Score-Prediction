{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Proprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from ../data/raw/train.csv\n",
      "Loading test data from ../data/raw/test.csv\n",
      "Train shape: (61609, 62)\n",
      "Test shape: (41074, 61)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils.utils import load_config\n",
    "from src.data.load_data import load_data\n",
    "\n",
    "config = load_config('../configs/config.yaml')\n",
    "train_df, test_df = load_data(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Impute using KNN and Mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 52 numerical columns and 9 categorical columns for imputation.\n",
      "\n",
      "[INFO] Starting KNN imputation for numerical features...\n",
      "[INFO] KNN imputation for numerical features completed in 641.62 seconds.\n",
      "\n",
      "[INFO] Starting mode imputation for categorical features...\n",
      "[INFO] Mode imputation for categorical features completed in 0.27 seconds.\n",
      "\n",
      "[INFO] Top columns with missing values in train after imputation:\n",
      "id                     0\n",
      "emotional_charge_2     0\n",
      "groove_efficiency_1    0\n",
      "beat_frequency_1       0\n",
      "organic_texture_2      0\n",
      "composition_label_0    0\n",
      "harmonic_scale_1       0\n",
      "intensity_index_0      0\n",
      "duration_ms_0          0\n",
      "album_name_length      0\n",
      "dtype: int64\n",
      "\n",
      "[INFO] Top columns with missing values in test after imputation:\n",
      "id                     0\n",
      "emotional_charge_2     0\n",
      "groove_efficiency_1    0\n",
      "beat_frequency_1       0\n",
      "organic_texture_2      0\n",
      "composition_label_0    0\n",
      "harmonic_scale_1       0\n",
      "intensity_index_0      0\n",
      "duration_ms_0          0\n",
      "album_name_length      0\n",
      "dtype: int64\n",
      "\n",
      "[INFO] Saving preprocessed datasets...\n",
      "[INFO] DataFrame saved to ../data/processed\\train_processed.csv\n",
      "[INFO] DataFrame saved to ../data/processed\\test_processed.csv\n",
      "[INFO] Preprocessed datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "import time\n",
    "from src.utils.utils import load_config, save_dataframe\n",
    "import os\n",
    "\n",
    "# 1. Identify numerical and categorical columns\n",
    "num_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "if 'target' in num_cols:\n",
    "    num_cols.remove('target')\n",
    "\n",
    "print(f\"Identified {len(num_cols)} numerical columns and {len(cat_cols)} categorical columns for imputation.\")\n",
    "\n",
    "# 2. KNN Imputation for numerical columns\n",
    "print(\"\\n[INFO] Starting KNN imputation for numerical features...\")\n",
    "start_time = time.time()\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
    "train_df_imputed = train_df.copy()\n",
    "test_df_imputed = test_df.copy()\n",
    "\n",
    "train_df_imputed[num_cols] = imputer.fit_transform(train_df[num_cols])\n",
    "test_df_imputed[num_cols] = imputer.transform(test_df[num_cols])\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"[INFO] KNN imputation for numerical features completed in {elapsed:.2f} seconds.\")\n",
    "\n",
    "# 3. Mode imputation for categorical columns\n",
    "print(\"\\n[INFO] Starting mode imputation for categorical features...\")\n",
    "start_time = time.time()\n",
    "for col in cat_cols:\n",
    "    mode = train_df[col].mode()[0]\n",
    "    train_df_imputed[col] = train_df[col].fillna(mode)\n",
    "    test_df_imputed[col] = test_df[col].fillna(mode)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"[INFO] Mode imputation for categorical features completed in {elapsed:.2f} seconds.\")\n",
    "\n",
    "# 4. Check missing values after imputation\n",
    "print(\"\\n[INFO] Top columns with missing values in train after imputation:\")\n",
    "print(train_df_imputed.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\n[INFO] Top columns with missing values in test after imputation:\")\n",
    "print(test_df_imputed.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# 5. Save preprocessed data\n",
    "print(\"\\n[INFO] Saving preprocessed datasets...\")\n",
    "config = load_config('../configs/config.yaml')\n",
    "processed_dir = config['data']['processed_dir']\n",
    "\n",
    "# Define save paths\n",
    "train_save_path = os.path.join(processed_dir, \"train_processed.csv\")\n",
    "test_save_path = os.path.join(processed_dir, \"test_processed.csv\")\n",
    "\n",
    "# Save DataFrames\n",
    "save_dataframe(train_df_imputed, train_save_path)\n",
    "save_dataframe(test_df_imputed, test_save_path)\n",
    "\n",
    "print(\"[INFO] Preprocessed datasets saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scaling the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DataFrame saved to ../data/processed\\train_standardized.csv\n",
      "[INFO] DataFrame saved to ../data/processed\\test_standardized.csv\n",
      "[INFO] Standardized datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from src.data.preprocess import DataStandardizer\n",
    "from src.utils.utils import load_config, save_dataframe\n",
    "import os\n",
    "\n",
    "# 1. Initialize and fit on train\n",
    "standardizer = DataStandardizer()\n",
    "train_standardized = standardizer.fit_transform(train_df_imputed, target_col='target')\n",
    "test_standardized = standardizer.transform(test_df_imputed)\n",
    "\n",
    "# 2. Save standardized data\n",
    "config = load_config('../configs/config.yaml')\n",
    "processed_dir = config['data']['processed_dir']\n",
    "train_save_path = os.path.join(processed_dir, \"train_standardized.csv\")\n",
    "test_save_path = os.path.join(processed_dir, \"test_standardized.csv\")\n",
    "\n",
    "save_dataframe(train_standardized, train_save_path)\n",
    "save_dataframe(test_standardized, test_save_path)\n",
    "\n",
    "print(\"[INFO] Standardized datasets saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encode Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'composition_label_0',\n",
    "    'composition_label_1',\n",
    "    'publication_timestamp',\n",
    "    'weekday_of_release',\n",
    "    'season_of_release',\n",
    "    'lunar_phase',\n",
    "    'creator_collective',\n",
    "    'composition_label_2',\n",
    "    'track_identifier'\n",
    "]\n",
    "\n",
    "high_card_cols = [\n",
    "    'composition_label_0',\n",
    "    'composition_label_1',\n",
    "    'composition_label_2',\n",
    "    'creator_collective',\n",
    "    'track_identifier'\n",
    "]\n",
    "\n",
    "low_card_cols = [\n",
    "    'weekday_of_release',\n",
    "    'season_of_release',\n",
    "    'lunar_phase'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Smooth Target Encording for High-Cardinality Columns + One Hot Encoding for Low-Cardinality Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from ../data/processed/train_processed.csv\n",
      "Loading test data from ../data/processed/test_processed.csv\n",
      "Train shape: (61609, 62)\n",
      "Test shape: (41074, 61)\n",
      "[INFO] DataFrame saved to ../data/processed\\train_encoded.csv\n",
      "[INFO] DataFrame saved to ../data/processed\\test_encoded.csv\n",
      "[INFO] Encoded datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from src.data.preprocess import SmoothedTargetEncoder\n",
    "from src.data.load_data import load_data\n",
    "from src.utils.utils import save_dataframe\n",
    "import os\n",
    "\n",
    "train_df, test_df = load_data(train_path=\"../data/processed/train_processed.csv\",\n",
    "                              test_path=\"../data/processed/test_processed.csv\"\n",
    "                              )\n",
    "\n",
    "target_col = 'target'\n",
    "\n",
    "# Apply smoothed target encoding\n",
    "for col in high_card_cols:\n",
    "    encoder = SmoothedTargetEncoder(column=col, smoothing=10)\n",
    "    train_df = encoder.fit_transform(train_df, train_df[target_col])\n",
    "    test_df = encoder.transform(test_df)\n",
    "    \n",
    "# Apply OHE \n",
    "train_df = pd.get_dummies(train_df, columns=low_card_cols)\n",
    "test_df = pd.get_dummies(test_df, columns=low_card_cols)\n",
    "\n",
    "# Align columns to ensure train and test have the same features\n",
    "train_df, test_df = train_df.align(test_df, join='left', axis=1, fill_value=0)\n",
    "test_df = test_df.drop(columns=['target'])\n",
    "\n",
    "\n",
    "# Convert boolean columns to integers (0/1)\n",
    "bool_cols_train = train_df.select_dtypes(include='bool').columns\n",
    "train_df[bool_cols_train] = train_df[bool_cols_train].astype(int)\n",
    "bool_cols_test = test_df.select_dtypes(include='bool').columns\n",
    "test_df[bool_cols_test] = test_df[bool_cols_test].astype(int)\n",
    "\n",
    "# Save the encoded data\n",
    "config = load_config('../configs/config.yaml')\n",
    "processed_dir = config['data']['processed_dir']\n",
    "\n",
    "train_save_path = os.path.join(processed_dir, \"train_encoded.csv\")\n",
    "test_save_path = os.path.join(processed_dir, \"test_encoded.csv\")\n",
    "\n",
    "save_dataframe(train_df, train_save_path)\n",
    "save_dataframe(test_df, test_save_path)\n",
    "\n",
    "print(\"[INFO] Encoded datasets saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61609 entries, 0 to 61608\n",
      "Data columns (total 74 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   id                            61609 non-null  float64\n",
      " 1   emotional_charge_2            61609 non-null  float64\n",
      " 2   groove_efficiency_1           61609 non-null  float64\n",
      " 3   beat_frequency_1              61609 non-null  float64\n",
      " 4   organic_texture_2             61609 non-null  float64\n",
      " 5   harmonic_scale_1              61609 non-null  float64\n",
      " 6   intensity_index_0             61609 non-null  float64\n",
      " 7   duration_ms_0                 61609 non-null  float64\n",
      " 8   album_name_length             61609 non-null  float64\n",
      " 9   beat_frequency_0              61609 non-null  float64\n",
      " 10  beat_frequency_2              61609 non-null  float64\n",
      " 11  artist_count                  61609 non-null  float64\n",
      " 12  publication_timestamp         61609 non-null  object \n",
      " 13  album_component_count         61609 non-null  float64\n",
      " 14  emotional_charge_1            61609 non-null  float64\n",
      " 15  emotional_charge_0            61609 non-null  float64\n",
      " 16  tonal_mode_2                  61609 non-null  float64\n",
      " 17  key_variety                   61609 non-null  float64\n",
      " 18  performance_authenticity_2    61609 non-null  float64\n",
      " 19  performance_authenticity_0    61609 non-null  float64\n",
      " 20  time_signature_1              61609 non-null  float64\n",
      " 21  duration_ms_2                 61609 non-null  float64\n",
      " 22  instrumental_density_2        61609 non-null  float64\n",
      " 23  organic_texture_0             61609 non-null  float64\n",
      " 24  vocal_presence_2              61609 non-null  float64\n",
      " 25  tonal_mode_1                  61609 non-null  float64\n",
      " 26  vocal_presence_1              61609 non-null  float64\n",
      " 27  vocal_presence_0              61609 non-null  float64\n",
      " 28  intensity_index_1             61609 non-null  float64\n",
      " 29  organic_immersion_0           61609 non-null  float64\n",
      " 30  tonal_mode_0                  61609 non-null  float64\n",
      " 31  groove_efficiency_2           61609 non-null  float64\n",
      " 32  instrumental_density_1        61609 non-null  float64\n",
      " 33  organic_immersion_2           61609 non-null  float64\n",
      " 34  duration_consistency          61609 non-null  float64\n",
      " 35  organic_texture_1             61609 non-null  float64\n",
      " 36  rhythmic_cohesion_0           61609 non-null  float64\n",
      " 37  emotional_resonance_1         61609 non-null  float64\n",
      " 38  rhythmic_cohesion_1           61609 non-null  float64\n",
      " 39  performance_authenticity_1    61609 non-null  float64\n",
      " 40  tempo_volatility              61609 non-null  float64\n",
      " 41  organic_immersion_1           61609 non-null  float64\n",
      " 42  groove_efficiency_0           61609 non-null  float64\n",
      " 43  emotional_resonance_2         61609 non-null  float64\n",
      " 44  time_signature_0              61609 non-null  float64\n",
      " 45  duration_ms_1                 61609 non-null  float64\n",
      " 46  harmonic_scale_0              61609 non-null  float64\n",
      " 47  time_signature_2              61609 non-null  float64\n",
      " 48  rhythmic_cohesion_2           61609 non-null  float64\n",
      " 49  emotional_resonance_0         61609 non-null  float64\n",
      " 50  harmonic_scale_2              61609 non-null  float64\n",
      " 51  intensity_index_2             61609 non-null  float64\n",
      " 52  instrumental_density_0        61609 non-null  float64\n",
      " 53  target                        61609 non-null  int64  \n",
      " 54  composition_label_0_encoded   61609 non-null  float64\n",
      " 55  composition_label_1_encoded   61609 non-null  float64\n",
      " 56  composition_label_2_encoded   61609 non-null  float64\n",
      " 57  creator_collective_encoded    61609 non-null  float64\n",
      " 58  track_identifier_encoded      61609 non-null  float64\n",
      " 59  weekday_of_release_Friday     61609 non-null  int64  \n",
      " 60  weekday_of_release_Monday     61609 non-null  int64  \n",
      " 61  weekday_of_release_Saturday   61609 non-null  int64  \n",
      " 62  weekday_of_release_Sunday     61609 non-null  int64  \n",
      " 63  weekday_of_release_Thursday   61609 non-null  int64  \n",
      " 64  weekday_of_release_Tuesday    61609 non-null  int64  \n",
      " 65  weekday_of_release_Wednesday  61609 non-null  int64  \n",
      " 66  season_of_release_autumn      61609 non-null  int64  \n",
      " 67  season_of_release_spring      61609 non-null  int64  \n",
      " 68  season_of_release_summer      61609 non-null  int64  \n",
      " 69  season_of_release_winter      61609 non-null  int64  \n",
      " 70  lunar_phase_full              61609 non-null  int64  \n",
      " 71  lunar_phase_new               61609 non-null  int64  \n",
      " 72  lunar_phase_waning            61609 non-null  int64  \n",
      " 73  lunar_phase_waxing            61609 non-null  int64  \n",
      "dtypes: float64(57), int64(16), object(1)\n",
      "memory usage: 34.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('publication_timestamp', axis=1, inplace=True)\n",
    "test_df.drop('publication_timestamp', axis=1, inplace=True)\n",
    "\n",
    "train_df.to_csv('../data/processed/train_encoded.csv')\n",
    "test_df.to_csv('../data/processed/test_encoded.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
